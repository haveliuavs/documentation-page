{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation","text":"<p>The documentation is a compilation of research aimed at giving an intuitive explanation of concepts before starting to dive deep into concepts. You can go through this documentation to get a good idea, then use the resources provided to understand more about the concepts introduced in this.</p>"},{"location":"#about-us","title":"About Us","text":"<p>We are Haveli R&amp;D Pvt Ltd. The company is devoted to creating custom drone solutions for different applications. This research was conducted as a part of developing the GPS-denied Navigation, for tea estates. The research has ended with the conceptualization of a pipeline, whose components will be explained in the documentation and resources will be given to understand concepts in depth. </p>"},{"location":"Community/","title":"Community","text":"<p>This page is dedicated to documenting open-source communities that work in the Aerial Robotics space. A community can be used to your advantage by involving in the community, learning and giving back to it by contributing to open-source projects.</p> <p>Aerial Robotics discourse is the discourse for the aerial robotics community. The working group aims to work with robotics enthusiasts to focus on aerial robot platforms. They meet once in every two weeks. The links to the meeting will be updated on the discourse monthwise.</p> <p>ROS-Aerial communnity is the GitHub page where you can find the work and the subgroups that the community is working on.</p> <p>OSRF Discord Server is the discord group for all the discussion pertaining to Robot Operating System.</p> <p>Ardupilot Discord Server is the discord group for ArduPilot. The link provided is the landing page containing information about the operations of the community.</p>"},{"location":"Datasets/","title":"Datasets","text":""},{"location":"Datasets/#datasets-for-perception","title":"Datasets for Perception","text":"<p>It is always best to use well known benchmark datasets to evaluate and understand perception algorithms. Even when the sensor is not available physically, we can get live time-synchronised data, with the help of bagfiles as well.</p> <p>A good example for this is the treescopes dataset. It is an under the canopy forest dataset, taken using UAV\u2019s and mobile platforms. It boasts a very advanced sensor suite and the data has been processed using a LIO method to fuse the inertial and LiDAR data to give an accurate registered pointcloud, and further processed using RangeNet++ for bark and ground plane identification. Another such dataset is the M3ED dataset, which contains data from multiple kinds of sensors like LiDARs, event camera\u2019s, RTK-GPS, IMU and stereo cameras. </p> <p>For Visual SLAM and Visual Odometry there are datasets which contain images in frame by frame manner, along with the camera intrinsic. Some of the datasets are Monocular VO by TUM and EuRoC MAV dataset by ETH Zurich. For the testing of DSO, LDSO and SVO, these were the datasets that were being used.</p>"},{"location":"Datasets/#monocular-vo-by-tum","title":"Monocular VO by TUM","text":"<p>Monocular Visual Odometry is a dataset for testing and benchmarking Visual Odometry and Monocular SLAM methods. It contains aboout 50 sequences recorded across different type of environments. The speciality of the dataset is that it provides the photometric calibration of the camera, and also points to a method to photometrically calibrate images. When it comes to direct method, photometric calibration is an absolute game-changer and have proven to increase performance, especially in the case of Direct Sparse Odometry. </p> <p>The structure of the dataset provided is as followed:</p> <ul> <li>images.zip :  Contains the video in a frame by frame JPEG or PNG file</li> <li>camera.txt : Contains the intrinsic parameters of the camera, which can be obtained through calibration</li> <li>pcalib.txt : Contains a single row with 256 values, mapping [0..255] to the respective irradiance value, i.e. containing the discretized inverse response function.</li> <li>times.txt  : Contains exposure times of each frame as given by the sensor</li> <li>vignette.png : Contains the vignette as pixel wise attenuation factors. </li> <li>groundtruthSync.txt: Contains the time synchronizeed ground truth information for each camera pose, corresponding to the image timestamps.</li> </ul> <p>A direct photometric calibration method is being shared by the same research group. Online Photometric Calibration uses a Levenberg-Marquardt based optimization after modelling the vignetting and response functions. This can be modified either to be run with video sequences or with live camera. Depending on the mode of calibration chosen, we can save the inverse response function and the vignetting factors or can directly wire it to the VO or SLAM algorithm. </p>"},{"location":"Datasets/#euroc-mav-by-eth-zurich","title":"EuRoC MAV by ETH Zurich","text":"<p>The EuRoC dataset includes synchronised timestamped data from a stereo camera system and an IMU (Inertial Measurement Unit) in a variety of demanding outdoor and interior conditions. The structure of the dataset is as follows:</p> <ul> <li>images.zip A compressed archive of grayscale photos collected by the stereo camera system. The archive usually has two subfolders (left and right) for the left and right camera photos, respectively.</li> <li>imuX.txt (multiple files): Text files containing synchronised Inertial Measurement Unit (IMU) data. These files contain measurements such as accelerometer, gyroscope, and sometimes barometer readings at timestamps that match the camera photos.</li> <li>groundtruth.txt: A text file holding the ground truth information for each camera pose (position and orientation) during the recording sequence. This information is often delivered at timestamps that are consistent with the image and IMU data.</li> </ul> <p>Note that some versions might include additional data files, such as calibration files for the camera system or IMU. These files are essential for correcting sensor distortions and ensuring accurate measurements. In the version we used this was not in a usable format for DSO, therefore we had to convert the parameters had to converted to .txt format and with the right distortion parameters. While running it for benchmarking for DSO, only one of the files out of the left and right images were used.  </p>"},{"location":"Datasets/#creating-a-custom-dataset-for-visual-odometryslam","title":"Creating a custom dataset for Visual Odometry/SLAM","text":"<p>Custom dataset can be created in the MonoVO format, which is easier to run with different algorithms. All you need is the video of the environment using a monocular camera. The calibration of the camera is needed for the pose estimation and mapping. The K matrix is considered the geometric calibration of the camera. The K matrix can be obtained by using Zhang's calibration, which can be obtained from the MATLAB tutorial provided. Using the values create the camera.txt files in the format as given by this. </p> <p>It is always preferred to have the photometric calibration files as well. These are the pcalib.txt and the vignette.png files. For the algorithm to work with photometric parameters both these files have to be provided. If the exposure times can be found with the timestamp and the frame values, then the mono-dataset code can be used.</p> <p>If not the files can be generated by using the online photometric calibration code, just by sending in the camera.txt files. When passing the parameters make sure to send the resolution values, so that the vignette image is in the same resolution as the feed. Document 1.1 elaborates on the use of the above. </p>"},{"location":"Datasets/#treescopes","title":"Treescopes","text":"<p>The Treescope dataset, developed by the University of Pennsylvania, provides data for robotic precision agricultural and forestry operations, notably tree counting\u00a0and mapping. The Treescope dataset seeks to give academics and developers with resources that:</p> <ul> <li> <p>Develop\u00a0and refine algorithms for robotic tree counting and mapping: The dataset\u00a0provides high-quality LiDAR data with ground truth labels, enabling researchers to train and evaluate algorithms for automated tree detection and estimate of tree\u00a0metrics including position, height, and diameter.</p> </li> <li> <p>Advance\u00a0LiDAR-based SLAM (Simultaneous Localization and Mapping) in agricultural settings: The dataset can be used to create SLAM algorithms that work well in orchards and woods, where typical visual SLAM approaches may fail due to a lack of visual cues.</p> </li> <li> <p>Benchmarking robotic system performance in precision agriculture: By providing a standardised dataset, researchers may evaluate the performance of various robotic platforms and algorithms for tree management activities in agricultural settings.</p> </li> </ul> <p>The dataset can be classified into two categories:</p> <ol> <li> <p>Processed data: This category includes ROS bags (Robot Operating System message format), which contain:</p> <ul> <li>Ground-truth LiDAR odometry: Precise information about the robot's movement derived from LiDAR data and potentially adjusted with other sensors (IMU, GPS).</li> <li>Velocity-corrected\u00a0point cloud frames are 3D points that reflect the environment collected by the LiDAR sensor and have been corrected for sensor motion to increase accuracy.</li> <li>Semantically segmented point clouds: The point cloud data is further classified, with points labelled as being on the ground, tree stems, or in other relevant categories. This segmentation is carried out utilising a trained deep learning network RangeNet++.</li> </ul> </li> <li> <p>Raw Data: This category includes the unprocessed sensor data acquired during the recordings.</p> <ul> <li>ROS bags may contain raw LiDAR data, IMU (Inertial Measurement Unit) data, RGB-D camera data (colour image with depth information).</li> <li>Sensor Metadata: Separate files containing information about the sensor models, calibration parameters, and other details needed to analyse the raw sensor data.</li> </ul> </li> </ol>"},{"location":"Datasets/#resources-and-further-work","title":"Resources and Further work","text":"<ol> <li> <p>Awesome SLAM datasets is a github 1. page, which has ordered different datasets that can be used to test SLAM algorithms. It has even ordered datasets based on the environments it provides.</p> </li> <li> <p>Mapping Quality Evaluation of Monocular SLAM solutions for Micro Aerial Vehicles </p> <ol> <li>Implement Instructions for custom dataset with LSD-SLAM, ORB-SLAM2, and LDSO</li> </ol> </li> <li> <p>Step by Step Procedure to make the Trajectory Analysis Work will help in setting up a basic comparison and plots for understanding the performance.</p> </li> </ol>"},{"location":"DirectSparseOdometry/","title":"Direct Sparse Odometry","text":""},{"location":"DirectSparseOdometry/#how-did-it-conceptualize","title":"How did it conceptualize?","text":"<p>Direct Sparse Odometry originated as a body of work for Visual Odometry using Monocular camera and Direct methods. Previously they made a breakthrough in Direct Methods using LSD-SLAM, but eventually it was seen that the method was computationally very expensive. If this methods has to be deployed on hardware such as robots and handheld systems, they need to be lightweight. </p> <p>In case of DSO, they use a windowed optimization approach, where the pose calculation takes place over a window of 7 frames (set as a parameter). This is basically a real-time windowed bundle adjustment, and hence can be used to optimize the reprojection error and hence calculate the inverse depth and ultimately the pose and hence the odometry of the camera sensor.</p> <p>The work has extended over the years, with different additions, such as Loop Closure, Dynamic Marginalization and Saliency Maps. They aim to solve problems that are specific to the use of the algorithm in a efficient manner, and also overcome its shortcomings.</p>"},{"location":"DirectSparseOdometry/#fundamental-components-of-dso","title":"Fundamental Components of DSO","text":""},{"location":"DirectSparseOdometry/#pixel-selector-and-gradients","title":"Pixel Selector and Gradients","text":"<p>DSO heavily depends on getting the anchor points for tracking by using gradients of the image pixel intensities. The horizontal and vertical image gradients are squared and added to form a map, which has the same size of the image. This is then cut into blocks of 32 x 32 blocks and for each block, calculate their gradients into histograms, which helps to threshold easily.</p> <p></p> <p>A sliding window is used to calculate a local gradient within a neighbourhood, and then select points using a threshold value. After the keypoints are selected, then they are reprojected onto the next frame, bundle adjustment is then carried out to reduce the reprojection error defined, which parametrizes on inverse depth, to solve depth and pose estimation. </p> <p>Gradient Pixel Selection by Ran Cheng is a blog post written by an ex-researcher from McGill University, which explains the code part of this pixel selector concept, and how does it exactly emulate a convolution operation through the different levels of resolution (Pyramid Levels).</p>"},{"location":"DirectSparseOdometry/#photometric-error-formulation-and-parameterization","title":"Photometric Error formulation and parameterization","text":"Residual Pattern for SSD calculation <p>The photometric error function seeks to minimise the photometric difference between observations of the same 3D point across multiple frames. The photometric error for a point p in reference frame I_i, viewed in a target frame I_j, is defined as the weighted sum of squared differences (SSD) over a small neighbourhood of pixels N_p around the projected point.</p> E p j := \u2211 p \u2208 N p w p \u2016 ( I j [ p \u2032 ] \u2212 b j ) \u2212 t j e a j t i e a i ( I i [ p ] \u2212 b i ) \u2016 \u03b3 <p>The above is the energy function of the algorithm. It is to be noted that this implementation is parameterized on the inverse depth. To understand in depth about this concept, there is a link to the word document given in the Resources and References column, which contains links to relevant papers explaining the SSD calculation and the requirement for an inverse depth parameterization. </p>"},{"location":"DirectSparseOdometry/#challenges-for-the-method","title":"Challenges for the method","text":"<p>Non-linear optimisation algorithms are used to minimise photometric error while estimating camera positions and pixel depth. Direct approaches are effective in low-texture settings and can provide rich 3D reconstructions due to their ability to utilise a large amount of image data. Direct picture alignment approaches are highly sensitive to unmodeled artefacts including rolling shutter, camera auto exposure, and gain control.</p> <p>Camera sensors are far from perfect. The build of the camera makes it prone to a lot of distortions of images. These distortions have to be modelled so that we can use this information to undistort the images.</p>"},{"location":"DirectSparseOdometry/#ros-2-development","title":"ROS 2 development","text":"<p>Docker was used to create a container with Ubuntu 22.04 and ROS 2 installation on it. The version of ROS used was Humble Hawkskill. The issues initally was mainly towards bringing the syntax upto the version with which we can write a wrapper code. </p> <p>For DSO and LDSO, the algorithm reads and runs very similarly. Step-by-Step Procedure to Implement DSO and LDSO with C++17 is a guide, which provides a guide to run the algorithm with a dataset in the latest versions of code and distro's. </p> <p>Once this is setup properly, primarly the viewer Pangolin, which is a C++ based GUI, needs to be removed completely and our wrapper code needs to be written instead. The Output3DWrapper.h files shows the functions and their outputs, which can be used to extract the required data from the algorithm. Step by Step DSO with ROS 2+RViz enlists the step by step process to create a docker and work with a ROS 2 image and the development of the wrapper code.</p>"},{"location":"DirectSparseOdometry/#results","title":"Results","text":"<p>Results contains the results from runs using different VO datasets and custom datasets as well. The results from the ROS 2 development is also added to the same folder. </p>"},{"location":"DirectSparseOdometry/#challenges-and-future-works","title":"Challenges and Future Works","text":""},{"location":"DirectSparseOdometry/#different-dso-implementations","title":"Different DSO Implementations","text":"<p>Due to the above shortcomings there are many implementation of DSO with addition of many different elements to mitigate the results of some of the shortcomings</p> <p>DSO in Dynamic Environments</p> <p>Salient DSO</p> <p>LDSO- DSO with Loop Closure</p> <p>DSO with Dynamic Marginalization</p> <p>Deep Direct Visual Odometry</p> <p>Feature based Localization with Direct Visual Odometry</p>"},{"location":"DirectSparseOdometry/#resources-and-references","title":"Resources and References","text":"<p>Research-DSO contains resources to understand the fundamental concepts and building blocks deployed to build the DSO method. </p>"},{"location":"Perception/","title":"Perception and Vision","text":"<p>Perception is the fundamental concept of how a robot can understand the surrounding environment to derive a set of actions, which then further dictates the flow of control. There are various sensors that are used and fused to make the robot aware of its surroundings and the changes by its actions. The two important ones we\u2019ll be looking into will be the Camera sensor and LiDAR sensor.</p>"},{"location":"Perception/#camera-sensors","title":"Camera Sensors","text":"<p>Camera sensor is a primary sensor used in perception. It takes in light through the opening of the shutter of the sensor, and as the light enters, it is projected onto the light receptors called photosites. A very simple and fundamental model of a camera is the pinhole camera. Here the pinhole acts as a single photosite in a camera sensor, and the inverted image on the flat surface is basically the light information captured by the sensor.</p>"},{"location":"Perception/#types-of-camera-sensors","title":"Types of Camera Sensors","text":"<p>Strictly speaking wrt robot perception there are three main camera sensors that are being used, these are:</p> <ul> <li>Monocular: As the name suggests, it uses a single lens to capture image and videos.</li> <li>Stereo: Stereo uses two or more image sensors, which allows it to simulate human binocular vision</li> <li>RGBD: It is a type of depth camera that can output the RGB and Depth value as the output in real time. </li> </ul> <p>These camera sensors are fundamentally different in the way they provide the data for use to complete tasks. The algorithms used for perception therefore also differ depending on the data it recieves.  In Visual SLAM/ Visual Odometry, there are distinct SLAM/ Odometry methods which utilise Monocular, Stereo and RGB-D data. </p>"},{"location":"Perception/#camera-calibration","title":"Camera Calibration","text":"<p>Imagine we have a checkerboard and we take an image of it using a camera. Due to the imperfections of the camera sensor, the image might appear warped. The warped image can be fixed using the camera parameters which we obtain using camera calibration. One of the most popular method of camera calibration is Zhang\u2019s camera calibration method. This method helps you understand how the camera \"sees\" the checkerboard, fixing any distortions and allowing you to make accurate measurements from the image. Here's a breakdown: The most important component is the checkerboard. It offers a distinct pattern with corners that are simple to recognise (intersections of black and white squares).</p> <ul> <li> <p>Taking Pictures: You capture the checkerboard in many images from different angles. The more pictures, the better the calibration. </p> </li> <li> <p>Software Analysis: The photos that are taken are examined by software. It locates the checkerboard corners in every image, effectively producing a set of data points at the locations of the corners.</p> </li> <li> <p>The actual dimensions and distances between the checkerboard squares are known to us. The \"ground truth\" data is essential. </p> </li> <li> <p>The program makes a comparison between the dimensions of the checkerboard that are known and the way the camera captured them in the photographs. This shows the distortions from the imperfect camera lens.</p> </li> <li> <p>Creating a Model: The programme creates a mathematical model that explains how the image is projected onto the camera sensor from the real world\u2014the checkerboard. This model considers factors like:</p> <ul> <li>Focal Length: It is the distance between the camera sensor and the optical centre of the lens, where the light information is recorded. </li> <li>Distortion: The way a lens causes straight lines in the actual world to look bent in an image.</li> <li>Center of Projection: The principal point where light rays converge on the sensor.</li> </ul> </li> </ul> <p>Consider it this way: In essence, what you're doing here is mapping out the real world (checkerboard) to the distorted picture (image) produced by the camera.This map allows you to interpret the image with greater accuracy. </p> <p>Some of the research that was done for Camera Calibration is documented in Research in Camera Calibration folder</p>"},{"location":"Perception/#tutorial-using-matlab-app-for-camera-calibration","title":"Tutorial using MATLAB app for Camera Calibration","text":"<p>Zhang's Calibration method with Tutorial</p>"},{"location":"Perception/#camera-models-used-for-sfm-and-slam","title":"Camera Models used for SFM and SLAM","text":"<p>The camera model will help in undistorting the images taken, which can help massively in undistorting the images. Moreover they are mathematical representations of the complexities that accompany with a real camera. Camera models such as pinhole, FOV, Rad-Tan, and Equidistant offer varied degrees of complexity in representing the relationship between the 3D world and a camera's 2D image. Understanding their capabilities and limitations is critical for jobs involving SfM, SLAM, and other applications that require precise image interpretation.</p> <p>Pinhole Camera: Assumes a straight line projection between a 3D point in the world frame to a correcponding 2D point in the image plane. This is the most fundamental model.</p> <p>FOV Model: Extends the pinhole camera by including the field of view of the camera, ie. defines the angular extent of the scene captured by the image sensor.</p> <p>Radial-Tangential Model: This model improves the pinhole model by incorporating radial distortion generated by the camera lens. Lenses frequently bend light slightly, causing straight lines in the actual world to look curved in the image, particularly near edges. These are caused by the imperfections in the lens manufacturing. </p> <p>Equidistant Model: This model is specifically designed for fisheye lenses, which capture an extremely wide field of view (often close to 180 degrees). Straight lines in a fisheye image become more bent as they go out from the image centre. The equidistant model maps 3D locations onto the picture plane using a precise mathematical model, which accounts for the high distortion.</p>"},{"location":"Perception/#lidar","title":"LiDAR","text":"<p>Light Detection And Ranging (LiDAR) is a sensor that uses light pulses, which reflects off surfaces and return to the sensor. It uses the time taken for each pulse to return to the sensor to calculate the distance. LiDAR sensor is usually used in robotics to obtain dense point cloud of the surrounding, which is then registered and corrected with IMU data using SLAM or SLOAM methods. The maps generated can be used for providing spatial information in 3-dimensions to UAVs whose DoF is 6.  Some of the different types of LiDAR sensor available are Mechanical, Hybrid Solid State and  Optical Phased Array. You can use this link to understand more about the different types of LiDAR sensor, based on its construction. </p>"},{"location":"Perception/#datasets-for-perception","title":"Datasets for Perception","text":"<p>It is always best to use well known benchmark datasets to evaluate and understand perception algorithms. Even when the sensor is not available physically, we can get live time-synchronised data, with the help of bagfiles as well.</p> <p>A good example for this is the treescopes dataset. It is an under the canopy forest dataset, taken using UAV\u2019s and mobile platforms. It boasts a very advanced sensor suite and the data has been processed using a LIO method to fuse the inertial and LiDAR data to give an accurate registered pointcloud, and further processed using RangeNet++ for bark and ground plane identification. Another such dataset is the M3ED dataset, which contains data from multiple kinds of sensors like LiDARs, event camera\u2019s, RTK-GPS, IMU and stereo cameras. </p> <p>For Visual SLAM and Visual Odometry there are datasets which contain images in frame by frame manner, along with the camera intrinsic. Some of the datasets are Monocular VO by TUM and EuRoC MAV dataset by ETH Zurich. For the testing of DSO, LDSO and SVO, these were the datasets that were being used.</p>"},{"location":"Perception/#resources-and-references","title":"Resources and References","text":""},{"location":"RobotOperatingSystem/","title":"Robot Operating System","text":""},{"location":"RobotOperatingSystem/#ros-noetic","title":"ROS Noetic","text":"<p>ROS Noetic is the last version of ROS 1 which is soon reaching its EOL. Now the importance of this version is the relative ease in use. All the packages are maintained well and can be used easily and quickly. The build and runtime are relatively lower and easier to debug and resolve. </p> <p>For drones, the SITL using Ardupilot is much more easier in ROS Noetic, in comparison to ROS 2. This is due to a very established MAVROS package which utilizes the MAVLink protocol to communicate with the flight controller. This communication can be setup, by using a launch file called apm.launch.</p>"},{"location":"RobotOperatingSystem/#ros-2-humble","title":"ROS 2 Humble","text":"<p>ROS 2 is the newer version of the Robot Operating System, which uses a completely different communication protocol underneath. It uses something called DDS. Apart from these changes, it uses a different build system to build its package (ament_cmake, colcon), and also the syntax has also changed. The version is to give much more flexibility to the lifecycle management of the node. If all this sounds like gibberish and it is something you are interested in exploring, the links to the associated concepts will be provided. </p>"},{"location":"RobotOperatingSystem/#differences-between-the-both-while-implementation","title":"Differences between the both while implementation","text":"<p>The main difference that you can catch while looking is the change in the way the packages are built. ROS 2 uses another build system for CMake called ament_cmake. The command to build the package is <code>colcon build</code>, as opposed to <code>catkin build</code> in Noetic. The syntax has also changed quite a lot, talking purely from the perspective of C++ code. ROS 2 uses a lot of C++ 11 features.</p> <p>ROS 2 supports multiple nodes running in parallel, allowing to leverage multi-core processors, which is a bit better than ROS 1. QoS allows to configure the data flow, dictating sending and recieving messages. It includes message reliability, deadline, priority, which ensures message with higher priority can be delivered on time. This link will provide a detailed, point by point improvements/differences between ROS 1 and ROS 2.</p>"},{"location":"RobotOperatingSystem/#bagfile-problem","title":"Bagfile Problem","text":"<p>ROS 2 is not backwards compatible due to a lot of fundamental changes in its construction. Compared to ROS 1, which has its own serialisation format, ROS 2 rosbags offer more flexibilty. The main effect of the ROS 1 to ROS 2 rosbag format changes is that tools for ROS 1 bags may not be compatible with ROS 2 rosbags, which could have an influence on developer processes. </p> <p>The same rosbags that work in ROS Noetic, does not work with any ROS 2 option. The tools used to handle the data from the bagfiles are different, due to the different serialization format. </p>"},{"location":"RobotOperatingSystem/#future-work-suggestions","title":"Future Work Suggestions","text":"<ol> <li> <p>For the bagfile problem, there are solutions with ROS 2 Foxy and ROS Noetic. To convert the bagfile format use one of these method and create a docker based application to convert ROS 1 bagfiles to ROS 2 and then test it with latest ROS 2 distro's such as Humble and Iron.</p> </li> <li> <p>Creating plugins for RViz for Waypoint navigation taking inspiration from the CTU-MRS stack (developed using ROS Noetic) for ROS 2 systems. RViz Aerial Plugins can be used as the base to start with.</p> </li> <li> <p>Configuring a forest based 3D environment in Gazebo integrated with the ArduPilot SITL for ROS 2, for testing the sensor suite and navigation algorithms. </p> </li> <li> <p>Developing localization and planning algorithms based on the voxelization methods chosen. Complete utilization of 3D Occupancy Grids.</p> </li> <li> <p>Localization method using monocular camera sensors, with LiDAR generated maps.</p> </li> </ol>"},{"location":"SLAM/","title":"Introduction","text":""},{"location":"SLAM/#introduction","title":"Introduction","text":"<p>A problem we as intellectuals face is the conundrum whether the chicken came first or the egg. The chicken lays the egg, but the chicken must have come out of some egg, but where is that egg from. No idea. Now why this problem is relevant</p> <p>Consider a robot, something like Wall-E for imagination purposes, is exploring an unknown scrap yard. It has to  </p> <ul> <li>Build a map: Make a representation of the surroundings, landmarks and other useful places of interest and importance.  </li> <li>Localise itself: Locate itself within the environment.</li> </ul> <p>Well, the problem is not as easy as it seems. These tasks are actually dependent on each other. To accurately build a map of the environment, the robot needs to know where it is in the environment. (Where am I headed to? How far have I come?) To locate itself in the environment, it needs a reference map. (Where are the cones and tables, wrt to where I am?)</p> <p>So, this is a cyclic dependency problem, just like the chicken and egg problem. Luckily the problem has been solved, but now we have to understand how it is solved. To understand this, fundamental and in depth knowledge in calculus, computer vision, geometry, stochastic processes, optimization of functions and many more. The documentation is to make sure that an intuitive understanding can be provided to the concepts and provide resources to go ahead with in-depth reading and understanding.</p>"},{"location":"SLAM/#simultaneous-localization-and-mapping","title":"Simultaneous Localization and Mapping","text":"<p>The problem of localization and mapping can be solved in many different ways which is evident from a lot of different techniques of SLAM. The overall gist is as the follows:</p> <p>Use a prior information: Even without a map, external sensors can be used to estimate the initial position and provide estimate of movement The robot uses sensors such as LiDAR, camera\u2019s, IMU\u2019s or ultrasonic sensors. The data can be used to identify landmarks or distinct features.  Use the initial movement and the data from the sensor to generate a map, which optimises the more you explore and gather data. Gradually as the map gets more detailed, the robot can use this map to localise itself in the environment and determine its position. More precise position information can also improve the quality of the map in terms of accuracy.</p> <p>This continuous loop of exploration, measurement and refinement is a gist of what SLAM does in order to solve the problem. Within these steps there are a lot of components that work in tandem which gives us this output. Depending on the sensor and the method of SLAM chosen, this can vary. Nevertheless the underlying fundamental problem and logic are the same.</p> <p>Fundamentals of SLAM is the drive folder that contains resources for different concepts which will be briefly explained in the further sections. The folder SLAM Fundamentals contains all the resources and links to YouTube playlists to understand the basics of SLAM.</p>"},{"location":"SLAM/#different-type-of-slam","title":"Different Type of SLAM","text":"<p>At a rudimentary standpoint, let's classify SLAM based on the sensor used. There are SLAM methods that use LiDAR sensors and there are others that use camera sensors, therefore called Visual SLAM. </p> <p>It can be said that fundamentally the sensor that is used to perceive depth is different in each case. The basic overall flow for SLAM is very similar, but depending on the sensor used there are workarounds done to make it work.</p> <p>A very good intuitive understanding about the fundamentals of SLAM can be found in the book SLAM for Dummies, which explains concepts in the LiDAR based SLAM approach. </p> <p>There will also be a dedicated section to visual SLAM, its classification and its fundamentals, since more in-depth research is done in that space.</p> <p>A brief of what we classify as the frontend and the backend of a SLAM method will be given in the next few sections.</p>"},{"location":"SLAM/#components-of-slam","title":"Components of SLAM","text":"<p>SLAM on a very broad basis can be split into frontend and backend. The frontend focuses on collecting anchor points or landmarks to be tracked for optimization, which will be done in the backend of the algorithm. Overall, SLAM's frontend-backend design allows for quick sensor data processing, reliable feature extraction and tracking, and continuous map and robot pose\u00a0refining. This enables robots to navigate unknown\u00a0regions while simultaneously creating a map of their surroundings in real time.</p>"},{"location":"SLAM/#frontend","title":"Frontend","text":"<p>Frontend of the algorithm purely handles the raw data from the sensors. These may involve:</p> <ul> <li>Preprocessing: This involves synchronization of data from multiple sensors (if involved), and correcting for distortions in the sensor data. </li> <li>Feature or Points extraction: Identifying landmarks or features from the perception data. These can be corners, edges, specific patterns (Indirect) or points and intensity values (Direct). </li> <li>Data Association: Creating initial correspondences between features observed in one sensor frame and those observed in succeeding frames. This entails identifying probable matches that could represent the same landmark or point in the environment from various perspectives.</li> </ul>"},{"location":"SLAM/#backend","title":"Backend","text":"<p>Backend uses the feature correspondence from the frontend of the algorithm and optimize to build a robust representation of the environment and estimate the pose of the sensor.</p> <ul> <li> <p>Pose\u00a0Estimation: Based on sensor data, feature correspondences, and the current map, the backend calculates the robot's most likely pose\u00a0(position and orientation) for each sensor frame. This entails addressing optimisation challenges to reduce the discrepancy between observed and expected sensor measurements. Adding new features to the map if they are consistently tracked and deemed reliable. Removing outdated information from the map to maintain accuracy and efficiency.</p> </li> <li> <p>Data Association Refinement: The backend refines the frontend's initial feature associations. It uses techniques such as outlier rejection and statistical analysis to ensure that features are accurately matched between sensor frames. This reduces false positives and improves the overall accuracy of pose estimation and mapping.</p> </li> <li> <p>Loop Closure Detection: In some SLAM techniques, the backend can identify loops. This refers to scenarios in which the robot returns to a previously investigated location. Loop closure aids in detecting and correcting cumulative errors\u00a0in map and robot pose estimation over time. By recognising loop closures, the algorithm can ensure consistency between previous and current observations, resulting in a more scale\u00a0accurate global map.</p> </li> </ul>"},{"location":"SLAM/#lidar-odometry-and-mapping-loam","title":"LiDaR Odometry and Mapping (LOAM)","text":"<p>LOAM is a method of LiDAR SLAM, which excels at real-time processing of LiDAR data and can generate a high-resolution 3D map. The actual idea of coming up with this method is the fundamental decomposition of the SLAM problem. One algorithm estimates the lidar's velocity by performing odometry with a high frequency but low fidelity. For the purpose of fine matching and point cloud registration, a different algorithm operates at a frequency that is an order of a magnitude lower.</p> <p>The paper LOAM: Lidar Odometry and Mapping in Real-time provides an insight into the basic structure and capabilities of the algorithm. Additional experimentation had been done and there are multiple modification done on this algorithm and have vaious spinoff implementations. One such implementation is SLOAM by UPenn.</p>"},{"location":"SLAM/#resources-and-references","title":"Resources and References","text":"<p>SLAM for Dummies</p> <p>Cyril Stachniss Fundamentals of SLAM</p> <p>SLAM Resources ordered is a github link to all the resources regarding SLAM in a definite and sorted order.</p>"},{"location":"VI-SLAM/","title":"VI-SLAM","text":""},{"location":"VI-SLAM/#visual-slam","title":"Visual SLAM","text":"<p>Visual SLAM is a type of SLAM that used a camera in its frontend to extract points and landmarks. There are different types of V-SLAM based on the camera sensor that is being used. The SLAM method that uses a single camera is called a Monocular SLAM. The cost of a monocular camera sensor is low and hence it is a very popular method amongst researchers. A photo is essentially a projection of a scene onto a camera\u2019s imaging plane. A monocular camera's image is a 2D projection of 3D space. To restore the 3D structure, alter the camera's view angle. Monocular SLAM follows the same principle. We move the camera and estimate its motion, as well as the distances and sizes of the objects in the image, so determining the scene's structure.  </p> <p>In monocular SLAM, depth can only be estimated by translational movement, and the true scale cannot be determined. These two factors potentially provide significant obstacles when implementing monocular SLAM in real-world applications. So, to achieve real-scaled depth, we begin to use stereo and RGB-D cameras. </p> <p>A stereo camera consists of two synchronised monocular cameras separated by a specified distance, called the baseline. Because the actual distance to the baseline is known, we can calculate each pixel's 3D position in a manner similar to human vision.  The downside of stereo cameras or multi-camera systems is that the configuration and calibration processes are complex. Their depth range and accuracy are constrained by the baseline length and camera resolution. Furthermore, stereo matching and disparity calculation use a lot of processing resources and typically require GPU or FPGA acceleration to build real-time depth maps.</p> <p>Depth cameras, commonly referred to as RGB-D cameras, have become increasingly popular since 2010. Similar to laser scanners, RGB-D cameras use infrared light structure or Time-of-Flight (ToF) principles to determine the distance between objects and the camera by actively generating light and receiving the returned light. The distance is not calculated on software, like for example using a stereo camera, but by using physical sensors instead which reduces the computational power required. But for SLAM purposes, RGB-D sensors doesnt prove effective, and hence are mainly used for Indoor purposes.</p> <p>In the subsequent sections, there will be explainations about Monocular SLAM, and within Monocular SLAM, Direct and Indirect Methods.</p> <p>A Comprehensive Survey on SLAM Algorithms is a survey paper on different open-source SLAM algorithms, which can give you a basic idea of the different methods available. Some of the other popular methods are listed below</p> <ul> <li> <p>LSD SLAM</p> </li> <li> <p>PTAM</p> </li> <li> <p>DTAM</p> </li> </ul>"},{"location":"VI-SLAM/#visual-odometry","title":"Visual Odometry","text":"<p>Visual Odometry is a method to estimate the ego-motion of a system using only visual input. The methodology used to estimate postion and velocity using images are similar to Visual SLAM methods. The fundamental difference between the both, is the fact that V-SLAM does both localization and mapping and incorporates methods like loop-closure, sensor integration and map reusal inorder to improve the localization, which VO does not. Due to this reason, the VO algorithms run faster than SLAM, and the maps generate are error prone and tend to be less detailed.</p> <p>Some popular methods are:</p> <ul> <li> <p>SVO-Hybrid Method</p> </li> <li> <p>DSO is also another Monocular Visual Odometry method, which will be explained in detail in the Direct Sparse Odometry section.</p> </li> </ul>"},{"location":"VI-SLAM/#direct-vs-indirect","title":"Direct vs Indirect","text":"<p>Indirect methods relies on extracting and tracking features (e.g., corners, edges) from consecutive image frames. The main challenge of VO/VSLAM is estimating camera motion from nearby images. However, the image itself is a numerical matrix that encodes brightness (intensity)\u00a0and colour. The integers in the matrix are abstract, making it difficult to calculate motion directly at the pixel level. Hence, it is more convenient to perform the extraction of features which will remain costly even when the camera moves slightly. After this, it then matches corresponding features across frames to establish geometric relationships.Using these relationships and camera models the camera motion is estimated and concurrently reconstructs 3D scene structure. Some examples of features that can be extracted are SIFT and ORB. </p> <ul> <li>ORB-SLAM is a very prominent SLAM method that uses feature extraction from images.</li> <li>VINS Mono uses Harris Corner features and then uses them to perform the KLT Tracker to track the points across the frames.</li> </ul> <p>Some of the other Indirect Methods are listed below:</p> <ul> <li>Kimera</li> <li>VINS-Fusion</li> <li>OKVIS</li> <li>ORB-SLAM 2</li> </ul> <p>But unfortunately, feature based methods require really good methods for feature detection and tracking, which can be computationally expensive. The method also proves ineffective in environments where there are repeated features, like forests or blank walls. The feature technique does not make advantage of all available information. An image has hundreds of thousands of pixels, but only a few hundred feature points. Using simply feature points discards the majority of potentially important image information.</p> <p>Direct methods on the other hand operates directly on the image's intensity values (pixels). A corresponding reprojection error function is formulated. This error function is further optimized to get the camera pose estimation. Direct Sparse Odometry is a direct method, that has been explained in detail. As a matter of fact, a lot of the direct methods have very common base logic. </p> <p>Some other Direct Methods are</p> <ul> <li>LSD SLAM</li> <li>Direct Sparse Odometry</li> <li>ROVIO</li> </ul> <p>Direct Methods in the Fundamentals of SLAM folder contains the papers that explain the fundamentals of Direct SLAM with camera's with related concepts.</p>"},{"location":"VI-SLAM/#resources-and-references","title":"Resources and References","text":"<p>SLAMBook is an excellent resource for beginners and veterans alike. The book explains the math and the intricacies behind the concepts that go into building a visual SLAM method. Anyone who wants to learn about Visual SLAM have to go through this book to understand the concepts at a very rudimentary level.</p> <p>Visual Navigation for Flying Robots consists of video lectures that explains all the concepts pertaining to the navigation of a robot, from mapping, localization to path planning.</p>"},{"location":"Voxelization/","title":"Voxelization","text":""},{"location":"Voxelization/#voxelization_1","title":"Voxelization","text":""},{"location":"Voxelization/#why-do-we-need-voxelization","title":"Why do we need Voxelization","text":"<p>The volumetric quantization of 3D space is important. We can use the information for navigational tasks, such as obstacle avoidance. Getting the information of the free and occupied volumes can be leveraged by obstacle avoidance algorithms. Discretizing the space can help us embed more information into the voxel. Methods that are mentioned below are some popular methods which were compared based on a lot of different metrics</p>"},{"location":"Voxelization/#octomap","title":"Octomap","text":"<p>Octomap uses an octree data structure for voxelization. An octree recursively divides space into eight octants (cubes). Depending on the resolution and likelihood of occupancy, each octant can be partitioned or represented as a single voxel.</p> <p>Occupancy likelihood: Octomap holds the likelihood that a voxel will be occupied by an obstacle in the range [0, 1]. 0 denotes open space, while 1 indicates fully occupied space. The values in between represent uncertainty.</p> <p>Efficiency: The octree structure facilitates the storage and retrieval of voxel information. Only occupied or partially occupied voxels are stored, resulting in lower memory utilisation in vast settings.</p>"},{"location":"Voxelization/#ufomap","title":"UFOMap","text":"<p>UFOMap, similar to Octomap, but fundamentally differs in the representation of the voxel occupancy probability. UFOMap builds on top of OctoMap's structure but utilizes a slightly different node type. It introduces the concept of an \"unknown\" state for each voxel, unlike OctoMap, which only uses occupied and free states, thus the name Unknown Free Occupied(UFO) Map.</p> <p>Uncertainty is explicitly represented by UFOMap, allowing for a more nuanced representation of the environment, particularly in instances where sensor data is sparse or noisy. OctoMap uses an occupancy probability threshold to determine whether a voxel is free, which may not always be accurate.</p> <p>Memory Efficiency: Because of the explicit representation of the unknown state, UFOMap has higher memory efficiency than OctoMap, especially in contexts with a large amount of unknown space. This is because UFOMap nodes with only unknown children can be combined into a single node.</p> <p>Ray Casting: Both methods allow ray casting, which is required for collision detection and path planning. However, UFOMap may require slightly different logic to handle unknown voxels during ray casting than OctoMap's handling of occupancy probability.</p>"},{"location":"Voxelization/#voxblox","title":"Voxblox","text":"<p>Voxblox is a voxel-based approach for constructing 3D representations of the world that focuses on Euclidean Signed Distance Fields. To be specific, unlike probabilities which was commonly used in Octomap, it uses Truncated Signed Distance Fields within each voxel. </p>"},{"location":"Voxelization/#how-to-use-the-3d-occupancies-provided","title":"How to use the 3D Occupancies provided","text":"<p>The reason for voxelization must be very clear. The voxelized 3D space must be used to localize and plan. Depending on the computational efficiency, memory efficiency and the planning algorithm development, a method must be chosen. </p> <p>We did our experimentation on the Treescopes dataset and the PCD files generated from DSO. The observations were as follows:</p> <ul> <li>The algorithms required consistent values of TF data between the sensor and the world/map frame.</li> <li>While running with the PCD file, only Octomap would provide results. The other methods didnt give an output due to lack of TF data. </li> <li>Running with Treescope dataset:<ul> <li>Octomap provides reasonably good results in a faster time with a 5cm voxel resolution and 15m range of the sensor considered.</li> <li>UFOMap was extremely slow in providing results at times, and would mark most voxels with uncertainity as unknown, and would only label volumes as occupied, only within a certain radius.</li> <li>Voxblox provided a good representation of the 3D environment, which could be exported as an SDF file.  The method to further used the Signed Distance Field values were very unclear. Nonetheless there are methods which use the voxblox representation for path planning.</li> </ul> </li> </ul> <p>3D space digital reconstruction  is the folder, which contains the information about some of the documentation, results and related papers to the analysis of pointcloud voxelization.</p> <p>From the brief observations, it was decided to use the Octomap method, as the method was also available for the ROS 2 distro. This spreadsheet will provide the direct comparison between the 3 methods tested.</p>"},{"location":"Voxelization/#future-works","title":"Future works","text":"<p>A localization and path planning method must be developed to utilize the voxelized environment. There are very few methods that use the voxelized 3D occupancy map for localization. Robot Localization is a good package to start with how the octomap 3D Occupancy Grid can be utilized for different purposes.</p> <p>GraphPlanner uses Voxblox-based maps. This can be tested with a simulation environment to figure our how it works.</p> <p>The CTU-MRS UAV stack has a component called Octomap Planner this can be looked into for further development of the stack.</p>"}]}